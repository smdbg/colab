{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smdbg/colab/blob/main/Step_3_Vector_DB_open.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# pip install lancedb docling openai tiktoken python-dotenv transformers\n",
        "\n",
        "import lancedb\n",
        "import tiktoken\n",
        "from docling.chunking import HybridChunker\n",
        "from docling.document_converter import DocumentConverter\n",
        "from lancedb.embeddings import get_registry\n",
        "from lancedb.pydantic import LanceModel, Vector\n",
        "from openai import OpenAI\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "OPENAI_API_KEY = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# --- CUSTOM TOKENIZER WRAPPER ---\n",
        "class OpenAITokenizerWrapper(PreTrainedTokenizerBase):\n",
        "    \"\"\"Minimal wrapper for OpenAI's tokenizer to match HuggingFace interface.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, model_name: str = \"cl100k_base\", max_length: int = 8191, **kwargs\n",
        "    ):\n",
        "        super().__init__(model_max_length=max_length, **kwargs)\n",
        "        self.tokenizer = tiktoken.get_encoding(model_name)\n",
        "        self._vocab_size = self.tokenizer.max_token_value\n",
        "\n",
        "    def tokenize(self, text: str, **kwargs) -> List[str]:\n",
        "        return [str(t) for t in self.tokenizer.encode(text)]\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        return self.tokenize(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token: str) -> int:\n",
        "        return int(token)\n",
        "\n",
        "    def _convert_id_to_token(self, index: int) -> str:\n",
        "        return str(index)\n",
        "\n",
        "    # --- FIX: –î–æ–±–∞–≤—è–º–µ –ª–∏–ø—Å–≤–∞—â–∏—è –º–µ—Ç–æ–¥ __len__ ---\n",
        "    def __len__(self) -> int:\n",
        "        return self._vocab_size\n",
        "\n",
        "    def get_vocab(self) -> Dict[str, int]:\n",
        "        return dict(enumerate(range(self.vocab_size)))\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return self._vocab_size\n",
        "\n",
        "    def save_vocabulary(self, *args) -> Tuple[str]:\n",
        "        return ()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, *args, **kwargs):\n",
        "        return cls()\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "\n",
        "# 1. Setup Tokenizer\n",
        "# text-embedding-3-large –ø–æ–¥–¥—ä—Ä–∂–∞ 8191 —Ç–æ–∫–µ–Ω–∞\n",
        "tokenizer = OpenAITokenizerWrapper(model_name=\"cl100k_base\", max_length=8191)\n",
        "MAX_TOKENS = 8191\n",
        "\n",
        "# 2. Extract Data\n",
        "print(\"‚è≥ Converting PDF...\")\n",
        "converter = DocumentConverter()\n",
        "result = converter.convert(\"https://arxiv.org/pdf/2408.09869\")\n",
        "\n",
        "# 3. Chunking\n",
        "print(\"üî™ Chunking...\")\n",
        "chunker = HybridChunker(\n",
        "    tokenizer=tokenizer,\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    merge_peers=True,\n",
        ")\n",
        "\n",
        "chunk_iter = chunker.chunk(dl_doc=result.document)\n",
        "chunks = list(chunk_iter)\n",
        "print(f\"‚úÖ Created {len(chunks)} chunks.\")\n",
        "\n",
        "# 4. LanceDB Setup\n",
        "if os.path.exists(\"data/lancedb\"):\n",
        "    shutil.rmtree(\"data/lancedb\")\n",
        "\n",
        "db = lancedb.connect(\"data/lancedb\")\n",
        "\n",
        "# 5. OpenAI Embedding Function (Native)\n",
        "# LanceDB –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —â–µ –∏–∑–ø–æ–ª–∑–≤–∞ –∫–ª—é—á–∞ –æ—Ç os.environ\n",
        "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-large\")\n",
        "\n",
        "# 6. Schema Definition\n",
        "class ChunkMetadata(LanceModel):\n",
        "    filename: str | None\n",
        "    page_numbers: List[int] | None\n",
        "    title: str | None\n",
        "\n",
        "class Chunks(LanceModel):\n",
        "    text: str = func.SourceField()\n",
        "    vector: Vector(func.ndims()) = func.VectorField()\n",
        "    metadata: ChunkMetadata\n",
        "\n",
        "table = db.create_table(\"docling\", schema=Chunks, mode=\"overwrite\")\n",
        "\n",
        "# 7. Insert Data\n",
        "print(\"üöÄ Indexing with OpenAI...\")\n",
        "processed_chunks = [\n",
        "    {\n",
        "        \"text\": chunk.text,\n",
        "        \"metadata\": {\n",
        "            \"filename\": chunk.meta.origin.filename if chunk.meta.origin else \"doc.pdf\",\n",
        "            \"page_numbers\": [\n",
        "                page_no\n",
        "                for page_no in sorted(\n",
        "                    set(\n",
        "                        prov.page_no\n",
        "                        for item in chunk.meta.doc_items\n",
        "                        for prov in item.prov\n",
        "                    )\n",
        "                )\n",
        "            ]\n",
        "            or None,\n",
        "            \"title\": chunk.meta.headings[0] if chunk.meta.headings else None,\n",
        "        },\n",
        "    }\n",
        "    for chunk in chunks\n",
        "]\n",
        "\n",
        "table.add(processed_chunks)\n",
        "\n",
        "# 8. Verify\n",
        "print(f\"üéâ Done! Rows indexed: {table.count_rows()}\")\n",
        "print(table.to_pandas().head(2))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2025-11-22 22:19:36,841 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Converting PDF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2025-11-22 22:19:36,916 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 22:19:36,917 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 22:19:37,330 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 22:19:37,335 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 22:19:37,337 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 22:19:37,460 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 22:19:37,543 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 22:19:37,544 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî™ Chunking...\n",
            "‚úÖ Created 21 chunks.\n",
            "üöÄ Indexing with OpenAI...\n",
            "üéâ Done! Rows indexed: 21\n",
            "                                                text  \\\n",
            "0  Christoph Auer Maksym Lysak Ahmed Nassar Miche...   \n",
            "1  This technical report introduces Docling , an ...   \n",
            "\n",
            "                                              vector  \\\n",
            "0  [0.005165622, -0.006144963, -0.021063859, 0.02...   \n",
            "1  [0.0008674973, 0.0135612255, -0.024147633, -0....   \n",
            "\n",
            "                                            metadata  \n",
            "0  {'filename': '2408.09869v5.pdf', 'page_numbers...  \n",
            "1  {'filename': '2408.09869v5.pdf', 'page_numbers...  \n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc1UQOuumEwN",
        "outputId": "13f11abc-3ed5-4aa1-9761-04d5dd284a37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lancedb\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Connect to the database\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "uri = \"data/lancedb\"\n",
        "db = lancedb.connect(uri)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Load the table\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "table = db.open_table(\"docling\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Search the table\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "result = table.search(query=\"–≥–∞–∑–æ–±–µ—Ç–æ–Ω–Ωa\", query_type=\"vector\").limit(3)\n",
        "result.to_pandas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "8-thQdLBp0XN",
        "outputId": "1aabec7f-809a-4513-aa95-b7d45b8833a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  –ù–∞—Å—Ç–æ—è—â–∞—Ç–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∏—Ä–∞ –∏–∑–∏—Å–∫–≤–∞–Ω–∏—è—Ç...   \n",
              "1  –ü—Ä–∏–≥–æ—Ç–≤—è–Ω–µ—Ç–æ –Ω–∞ –≥–∞–∑–æ–±–µ—Ç–æ–Ω–Ω–∞—Ç–∞ —Å–º–µ—Å –≤–∫–ª—é—á–≤–∞ —Å–ª–µ...   \n",
              "2  –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–∏—è —É—á–∞—Å—Ç—ä–∫, –∫—ä–¥–µ—Ç–æ —Å–µ –∏–∑–ø—ä–ª–Ω—è–≤–∞ –ø...   \n",
              "\n",
              "                                              vector  \\\n",
              "0  [0.027452853, -0.021154394, -0.0029410352, 0.0...   \n",
              "1  [0.034642067, -0.034101136, -0.009550798, 0.00...   \n",
              "2  [0.035472944, -0.0016953981, -0.006564234, 0.0...   \n",
              "\n",
              "                                            metadata  _distance  \n",
              "0  {'filename': 'BOFI 50014.1 –ü—Ä–∏–≥–æ—Ç–≤—è–Ω–µ –∏ –∑–∞–ø—ä–ª–≤...   0.948265  \n",
              "1  {'filename': 'BOFI 50014.1 –ü—Ä–∏–≥–æ—Ç–≤—è–Ω–µ –∏ –∑–∞–ø—ä–ª–≤...   0.953884  \n",
              "2  {'filename': 'BOFI 50014.1 –ü—Ä–∏–≥–æ—Ç–≤—è–Ω–µ –∏ –∑–∞–ø—ä–ª–≤...   0.980890  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d3860d2a-1ec9-4a6c-94ae-368fbb20f69b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>vector</th>\n",
              "      <th>metadata</th>\n",
              "      <th>_distance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>–ù–∞—Å—Ç–æ—è—â–∞—Ç–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∏—Ä–∞ –∏–∑–∏—Å–∫–≤–∞–Ω–∏—è—Ç...</td>\n",
              "      <td>[0.027452853, -0.021154394, -0.0029410352, 0.0...</td>\n",
              "      <td>{'filename': 'BOFI 50014.1 –ü—Ä–∏–≥–æ—Ç–≤—è–Ω–µ –∏ –∑–∞–ø—ä–ª–≤...</td>\n",
              "      <td>0.948265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>–ü—Ä–∏–≥–æ—Ç–≤—è–Ω–µ—Ç–æ –Ω–∞ –≥–∞–∑–æ–±–µ—Ç–æ–Ω–Ω–∞—Ç–∞ —Å–º–µ—Å –≤–∫–ª—é—á–≤–∞ —Å–ª–µ...</td>\n",
              "      <td>[0.034642067, -0.034101136, -0.009550798, 0.00...</td>\n",
              "      <td>{'filename': 'BOFI 50014.1 –ü—Ä–∏–≥–æ—Ç–≤—è–Ω–µ –∏ –∑–∞–ø—ä–ª–≤...</td>\n",
              "      <td>0.953884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–∏—è —É—á–∞—Å—Ç—ä–∫, –∫—ä–¥–µ—Ç–æ —Å–µ –∏–∑–ø—ä–ª–Ω—è–≤–∞ –ø...</td>\n",
              "      <td>[0.035472944, -0.0016953981, -0.006564234, 0.0...</td>\n",
              "      <td>{'filename': 'BOFI 50014.1 –ü—Ä–∏–≥–æ—Ç–≤—è–Ω–µ –∏ –∑–∞–ø—ä–ª–≤...</td>\n",
              "      <td>0.980890</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3860d2a-1ec9-4a6c-94ae-368fbb20f69b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d3860d2a-1ec9-4a6c-94ae-368fbb20f69b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d3860d2a-1ec9-4a6c-94ae-368fbb20f69b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8e87b022-84e7-4671-9e4c-772c548e13eb\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8e87b022-84e7-4671-9e4c-772c548e13eb')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8e87b022-84e7-4671-9e4c-772c548e13eb button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"result\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"\\u041d\\u0430\\u0441\\u0442\\u043e\\u044f\\u0449\\u0430\\u0442\\u0430 \\u0438\\u043d\\u0441\\u0442\\u0440\\u0443\\u043a\\u0446\\u0438\\u044f \\u0440\\u0435\\u0433\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0438\\u0440\\u0430 \\u0438\\u0437\\u0438\\u0441\\u043a\\u0432\\u0430\\u043d\\u0438\\u044f\\u0442\\u0430 \\u043a\\u044a\\u043c \\u0441\\u044a\\u0441\\u0442\\u0430\\u0432\\u0430 \\u043d\\u0430 \\u0433\\u0430\\u0437\\u043e\\u0431\\u0435\\u0442\\u043e\\u043d\\u0430 \\u0438 \\u043f\\u0430\\u0440\\u0430\\u043c\\u0435\\u0442\\u0440\\u0438\\u0442\\u0435 \\u0437\\u0430 \\u043a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b \\u043d\\u0430 \\u043f\\u0440\\u043e\\u0446\\u0435\\u0441\\u0430 \\u043f\\u0440\\u0438 \\u043f\\u0440\\u0438\\u0433\\u043e\\u0442\\u0432\\u044f\\u043d\\u0435\\u0442\\u043e \\u043c\\u0443.\",\n          \"\\u041f\\u0440\\u0438\\u0433\\u043e\\u0442\\u0432\\u044f\\u043d\\u0435\\u0442\\u043e \\u043d\\u0430 \\u0433\\u0430\\u0437\\u043e\\u0431\\u0435\\u0442\\u043e\\u043d\\u043d\\u0430\\u0442\\u0430 \\u0441\\u043c\\u0435\\u0441 \\u0432\\u043a\\u043b\\u044e\\u0447\\u0432\\u0430 \\u0441\\u043b\\u0435\\u0434\\u043d\\u0438\\u0442\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0447\\u043d\\u0438 \\u043e\\u043f\\u0435\\u0440\\u0430\\u0446\\u0438\\u0438:\\n\\u0417\\u0410 \\u0421\\u041b\\u0423\\u0416\\u0415\\u0411\\u041d\\u041e \\u041f\\u041e\\u041b\\u0417\\u0412\\u0410\\u041d\\u0415\",\n          \"\\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u0435\\u043d\\u0438\\u044f \\u0443\\u0447\\u0430\\u0441\\u0442\\u044a\\u043a, \\u043a\\u044a\\u0434\\u0435\\u0442\\u043e \\u0441\\u0435 \\u0438\\u0437\\u043f\\u044a\\u043b\\u043d\\u044f\\u0432\\u0430 \\u043f\\u0440\\u0438\\u0433\\u043e\\u0442\\u0432\\u044f\\u043d\\u0435\\u0442\\u043e \\u0438 \\u0437\\u0430\\u043b\\u0438\\u0432\\u0430\\u043d\\u0435\\u0442\\u043e \\u043d\\u0430 \\u0433\\u0430\\u0437\\u043e\\u0431\\u0435\\u0442\\u043e\\u043d\\u0430 \\u0432 \\u043a\\u043e\\u043d\\u0441\\u0442\\u0440\\u0443\\u043a\\u0446\\u0438\\u044f\\u0442\\u0430 \\u043d\\u0430 \\u0438\\u0437\\u0434\\u0435\\u043b\\u0438\\u044f\\u0442\\u0430.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vector\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"_distance\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9482648968696594,\n          0.9538838267326355,\n          0.9808897376060486\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r download.zip data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXa6oO0irUyB",
        "outputId": "205ba4ef-b7d6-4f78-cbeb-0a95fa9b1278"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: data/ (stored 0%)\n",
            "  adding: data/lancedb/ (stored 0%)\n",
            "  adding: data/lancedb/docling.lance/ (stored 0%)\n",
            "  adding: data/lancedb/docling.lance/data/ (stored 0%)\n",
            "  adding: data/lancedb/docling.lance/data/10110010000100111110001155b97640cda0da2bfc6b1749c6.lance (deflated 25%)\n",
            "  adding: data/lancedb/docling.lance/_transactions/ (stored 0%)\n",
            "  adding: data/lancedb/docling.lance/_transactions/0-021bd0ab-4b76-4b9d-8ea6-46e168351cb1.txn (deflated 36%)\n",
            "  adding: data/lancedb/docling.lance/_transactions/1-43110221-7122-453a-8145-69687af7b38b.txn (deflated 9%)\n",
            "  adding: data/lancedb/docling.lance/_versions/ (stored 0%)\n",
            "  adding: data/lancedb/docling.lance/_versions/1.manifest (deflated 36%)\n",
            "  adding: data/lancedb/docling.lance/_versions/2.manifest (deflated 35%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import glob\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import lancedb\n",
        "import tiktoken\n",
        "from docling.chunking import HybridChunker\n",
        "from docling.document_converter import DocumentConverter\n",
        "from lancedb.embeddings import get_registry\n",
        "from lancedb.pydantic import LanceModel, Vector\n",
        "from openai import OpenAI\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "OPENAI_API_KEY = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "INPUT_FOLDER = \"INPUT\"\n",
        "DB_PATH = \"data/lancedb\"\n",
        "MAX_TOKENS = 8191\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# --- TOKENIZER WRAPPER ---\n",
        "class OpenAITokenizerWrapper(PreTrainedTokenizerBase):\n",
        "    def __init__(self, model_name: str = \"cl100k_base\", max_length: int = 8191, **kwargs):\n",
        "        super().__init__(model_max_length=max_length, **kwargs)\n",
        "        self.tokenizer = tiktoken.get_encoding(model_name)\n",
        "        self._vocab_size = self.tokenizer.max_token_value\n",
        "\n",
        "    def tokenize(self, text: str, **kwargs) -> List[str]:\n",
        "        return [str(t) for t in self.tokenizer.encode(text)]\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        return self.tokenize(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token: str) -> int:\n",
        "        return int(token)\n",
        "\n",
        "    def _convert_id_to_token(self, index: int) -> str:\n",
        "        return str(index)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self._vocab_size\n",
        "\n",
        "    def get_vocab(self) -> Dict[str, int]:\n",
        "        return dict(enumerate(range(self.vocab_size)))\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return self._vocab_size\n",
        "\n",
        "    def save_vocabulary(self, *args) -> Tuple[str]:\n",
        "        return ()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, *args, **kwargs):\n",
        "        return cls()\n",
        "\n",
        "# --- SCHEMA SETUP ---\n",
        "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-large\")\n",
        "\n",
        "class ChunkMetadata(LanceModel):\n",
        "    filename: str | None\n",
        "    page_numbers: List[int] | None\n",
        "    title: str | None\n",
        "\n",
        "class Chunks(LanceModel):\n",
        "    text: str = func.SourceField()\n",
        "    vector: Vector(func.ndims()) = func.VectorField()\n",
        "    metadata: ChunkMetadata\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "\n",
        "def get_or_create_table():\n",
        "    \"\"\"–°–≤—ä—Ä–∑–≤–∞ —Å–µ —Å –±–∞–∑–∞—Ç–∞.\"\"\"\n",
        "    db = lancedb.connect(DB_PATH)\n",
        "\n",
        "    if \"docling\" in db.table_names():\n",
        "        return db.open_table(\"docling\")\n",
        "    else:\n",
        "        print(\"üÜï –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –Ω–æ–≤–∞ —Ç–∞–±–ª–∏—Ü–∞ 'docling'...\")\n",
        "        return db.create_table(\"docling\", schema=Chunks)\n",
        "\n",
        "def process_single_file(file_path, table, tokenizer):\n",
        "    filename = os.path.basename(file_path)\n",
        "    print(f\"\\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞: {filename}\")\n",
        "\n",
        "    # --- DEDUPLICATION CHECK ---\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–≤–∞–º–µ –¥–∞–ª–∏ —Ñ–∞–π–ª—ä—Ç –≤–µ—á–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞\n",
        "    existing_files = table.search().where(f\"metadata.filename = '{filename}'\").limit(1).to_pandas()\n",
        "\n",
        "    if not existing_files.empty:\n",
        "        print(f\"   üîÑ –§–∞–π–ª—ä—Ç '{filename}' –≤–µ—á–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞. –ò–∑—Ç—Ä–∏–≤–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ä–∏—Ç–µ –∑–∞–ø–∏—Å–∏...\")\n",
        "        table.delete(f\"metadata.filename = '{filename}'\")\n",
        "        print(\"   üóëÔ∏è –°—Ç–∞—Ä–∏—Ç–µ –∑–∞–ø–∏—Å–∏ —Å–∞ –∏–∑—Ç—Ä–∏—Ç–∏. –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ –Ω–æ–≤–∞—Ç–∞ –≤–µ—Ä—Å–∏—è...\")\n",
        "    # ---------------------------\n",
        "\n",
        "    try:\n",
        "        # 1. Docling Convert\n",
        "        converter = DocumentConverter()\n",
        "        result = converter.convert(file_path)\n",
        "\n",
        "        # 2. Chunking\n",
        "        chunker = HybridChunker(\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            merge_peers=True,\n",
        "        )\n",
        "        chunks = list(chunker.chunk(dl_doc=result.document))\n",
        "        print(f\"   üß© –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–∏ {len(chunks)} –ø–∞—Ä—á–µ—Ç–∞.\")\n",
        "\n",
        "        if not chunks:\n",
        "            print(\"   ‚ö†Ô∏è –§–∞–π–ª—ä—Ç –µ –ø—Ä–∞–∑–µ–Ω –∏–ª–∏ –Ω–µ –º–æ–∂–µ –¥–∞ —Å–µ –ø—Ä–æ—á–µ—Ç–µ.\")\n",
        "            return False\n",
        "\n",
        "        # 3. Prepare Data\n",
        "        processed_chunks = [\n",
        "            {\n",
        "                \"text\": chunk.text,\n",
        "                \"metadata\": {\n",
        "                    \"filename\": filename,\n",
        "                    \"page_numbers\": [\n",
        "                        page_no for item in chunk.meta.doc_items for item_prov in item.prov for page_no in [item_prov.page_no]\n",
        "                    ] or None,\n",
        "                    \"title\": chunk.meta.headings[0] if chunk.meta.headings else None,\n",
        "                },\n",
        "            }\n",
        "            for chunk in chunks\n",
        "        ]\n",
        "\n",
        "        # 4. Insert into LanceDB\n",
        "        print(\"   üöÄ Indexing to LanceDB...\")\n",
        "        table.add(processed_chunks)\n",
        "        print(\"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–∞–Ω!\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå –ì–†–ï–®–ö–ê –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞—Ç–∞ –Ω–∞ —Ñ–∞–π–ª–∞: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- MAIN LOOP ---\n",
        "\n",
        "def main():\n",
        "    # 1. Setup\n",
        "    if not os.path.exists(INPUT_FOLDER):\n",
        "        os.makedirs(INPUT_FOLDER)\n",
        "        print(f\"üìÅ –°—ä–∑–¥–∞–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ '{INPUT_FOLDER}'. –°–ª–∞–≥–∞–π PDF —Ñ–∞–π–ª–æ–≤–µ –≤—ä—Ç—Ä–µ!\")\n",
        "\n",
        "    tokenizer = OpenAITokenizerWrapper(model_name=\"cl100k_base\", max_length=8191)\n",
        "    table = get_or_create_table()\n",
        "\n",
        "    print(f\"\\nüëÄ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞ –ø–∞–ø–∫–∞ '{INPUT_FOLDER}'...\")\n",
        "    print(\"–ù–∞—Ç–∏—Å–Ω–∏ Ctrl+C –∑–∞ —Å–ø–∏—Ä–∞–Ω–µ.\\n\")\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            pdf_files = glob.glob(os.path.join(INPUT_FOLDER, \"*.pdf\"))\n",
        "\n",
        "            if pdf_files:\n",
        "                print(f\"üîé –ù–∞–º–µ—Ä–µ–Ω–∏ {len(pdf_files)} —Ñ–∞–π–ª–∞ –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞!\")\n",
        "\n",
        "                for file_path in pdf_files:\n",
        "                    success = process_single_file(file_path, table, tokenizer)\n",
        "\n",
        "                    if success:\n",
        "                        try:\n",
        "                            os.remove(file_path)\n",
        "                            print(f\"   üóëÔ∏è –ò–∑—Ç—Ä–∏—Ç –æ—Ç Input: {os.path.basename(file_path)}\")\n",
        "                        except OSError as e:\n",
        "                            print(f\"   ‚ö†Ô∏è –ù–µ –º–æ–≥–∞ –¥–∞ –∏–∑—Ç—Ä–∏—è —Ñ–∞–π–ª–∞: {e}\")\n",
        "                    else:\n",
        "                        new_name = file_path + \".failed\"\n",
        "                        os.rename(file_path, new_name)\n",
        "                        print(f\"   ‚ö†Ô∏è –ü—Ä–µ–∏–º–µ–Ω—É–≤–∞–Ω –Ω–∞ {os.path.basename(new_name)}\")\n",
        "\n",
        "            time.sleep(10)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüõë –°–ø–∏—Ä–∞–Ω–µ –Ω–∞ —Å–∫—Ä–∏–ø—Ç–∞. –î–æ–≤–∏–∂–¥–∞–Ω–µ!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZPdflaDtOaq",
        "outputId": "e4c5702d-39dd-4a45-fa70-990b6e7d3483"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üëÄ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞ –ø–∞–ø–∫–∞ 'INPUT'...\n",
            "–ù–∞—Ç–∏—Å–Ω–∏ Ctrl+C –∑–∞ —Å–ø–∏—Ä–∞–Ω–µ.\n",
            "\n",
            "\n",
            "üõë –°–ø–∏—Ä–∞–Ω–µ –Ω–∞ —Å–∫—Ä–∏–ø—Ç–∞. –î–æ–≤–∏–∂–¥–∞–Ω–µ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import glob\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import lancedb\n",
        "import tiktoken\n",
        "from docling.chunking import HybridChunker\n",
        "from docling.document_converter import DocumentConverter\n",
        "from lancedb.embeddings import get_registry\n",
        "from lancedb.pydantic import LanceModel, Vector\n",
        "from openai import OpenAI\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "\n",
        "# --- LOGGING & WARNINGS SUPPRESSION ---\n",
        "import logging\n",
        "import warnings\n",
        "# –°–∫—Ä–∏–≤–∞–º–µ —Å–ø–∞–º–∞ –æ—Ç Docling –∏ RapidOCR\n",
        "logging.getLogger(\"docling\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"docling.backend.msword_backend\").setLevel(logging.ERROR) # –ó–∞ DOCX –≥—Ä–µ—à–∫–∏—Ç–µ\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "OPENAI_API_KEY = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "INPUT_FOLDER = \"INPUT\"\n",
        "DB_PATH = \"data/lancedb\"\n",
        "MAX_TOKENS = 8191\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# --- TOKENIZER WRAPPER ---\n",
        "class OpenAITokenizerWrapper(PreTrainedTokenizerBase):\n",
        "    def __init__(self, model_name: str = \"cl100k_base\", max_length: int = 8191, **kwargs):\n",
        "        super().__init__(model_max_length=max_length, **kwargs)\n",
        "        self.tokenizer = tiktoken.get_encoding(model_name)\n",
        "        self._vocab_size = self.tokenizer.max_token_value\n",
        "\n",
        "    def tokenize(self, text: str, **kwargs) -> List[str]:\n",
        "        return [str(t) for t in self.tokenizer.encode(text)]\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        return self.tokenize(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token: str) -> int:\n",
        "        return int(token)\n",
        "\n",
        "    def _convert_id_to_token(self, index: int) -> str:\n",
        "        return str(index)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self._vocab_size\n",
        "\n",
        "    def get_vocab(self) -> Dict[str, int]:\n",
        "        return dict(enumerate(range(self.vocab_size)))\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return self._vocab_size\n",
        "\n",
        "    def save_vocabulary(self, *args) -> Tuple[str]:\n",
        "        return ()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, *args, **kwargs):\n",
        "        return cls()\n",
        "\n",
        "# --- SCHEMA SETUP ---\n",
        "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-large\")\n",
        "\n",
        "class ChunkMetadata(LanceModel):\n",
        "    filename: str | None\n",
        "    # FIX: –ü—Ä–µ–º–∞—Ö–≤–∞–º–µ \"| None\". LanceDB –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞ –≤–∏–Ω–∞–≥–∏ List, –¥–æ—Ä–∏ –¥–∞ –µ –ø—Ä–∞–∑–µ–Ω.\n",
        "    # –¢–æ–≤–∞ –æ–ø—Ä–∞–≤—è –≥—Ä–µ—à–∫–∞—Ç–∞ \"pyarrow.lib.DataType object has no attribute value_field\"\n",
        "    page_numbers: List[int]\n",
        "    title: str | None\n",
        "\n",
        "class Chunks(LanceModel):\n",
        "    text: str = func.SourceField()\n",
        "    vector: Vector(func.ndims()) = func.VectorField()\n",
        "    metadata: ChunkMetadata\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "\n",
        "def get_or_create_table():\n",
        "    \"\"\"–°–≤—ä—Ä–∑–≤–∞ —Å–µ —Å –±–∞–∑–∞—Ç–∞.\"\"\"\n",
        "    db = lancedb.connect(DB_PATH)\n",
        "\n",
        "    if \"docling\" in db.table_names():\n",
        "        return db.open_table(\"docling\")\n",
        "    else:\n",
        "        print(\"üÜï –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –Ω–æ–≤–∞ —Ç–∞–±–ª–∏—Ü–∞ 'docling'...\")\n",
        "        return db.create_table(\"docling\", schema=Chunks)\n",
        "\n",
        "def process_single_file(file_path, table, tokenizer):\n",
        "    filename = os.path.basename(file_path)\n",
        "    print(f\"\\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞: {filename}\")\n",
        "\n",
        "    # --- DEDUPLICATION CHECK ---\n",
        "    existing_files = table.search().where(f\"metadata.filename = '{filename}'\").limit(1).to_pandas()\n",
        "\n",
        "    if not existing_files.empty:\n",
        "        print(f\"   üîÑ –§–∞–π–ª—ä—Ç '{filename}' –≤–µ—á–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞. –ò–∑—Ç—Ä–∏–≤–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ä–∏—Ç–µ –∑–∞–ø–∏—Å–∏...\")\n",
        "        table.delete(f\"metadata.filename = '{filename}'\")\n",
        "        print(\"   üóëÔ∏è –°—Ç–∞—Ä–∏—Ç–µ –∑–∞–ø–∏—Å–∏ —Å–∞ –∏–∑—Ç—Ä–∏—Ç–∏. –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ –Ω–æ–≤–∞—Ç–∞ –≤–µ—Ä—Å–∏—è...\")\n",
        "    # ---------------------------\n",
        "\n",
        "    try:\n",
        "        # 1. Docling Convert\n",
        "        converter = DocumentConverter()\n",
        "        result = converter.convert(file_path)\n",
        "\n",
        "        # 2. Chunking\n",
        "        chunker = HybridChunker(\n",
        "            tokenizer=tokenizer,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            merge_peers=True,\n",
        "        )\n",
        "        chunks = list(chunker.chunk(dl_doc=result.document))\n",
        "        print(f\"   üß© –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–∏ {len(chunks)} –ø–∞—Ä—á–µ—Ç–∞.\")\n",
        "\n",
        "        if not chunks:\n",
        "            print(\"   ‚ö†Ô∏è –§–∞–π–ª—ä—Ç –µ –ø—Ä–∞–∑–µ–Ω –∏–ª–∏ –Ω–µ –º–æ–∂–µ –¥–∞ —Å–µ –ø—Ä–æ—á–µ—Ç–µ.\")\n",
        "            return False\n",
        "\n",
        "        # 3. Prepare Data\n",
        "        processed_chunks = [\n",
        "            {\n",
        "                \"text\": chunk.text,\n",
        "                \"metadata\": {\n",
        "                    \"filename\": filename,\n",
        "                    \"page_numbers\": [\n",
        "                        page_no for item in chunk.meta.doc_items for item_prov in item.prov for page_no in [item_prov.page_no]\n",
        "                    ], # FIX: –ú–∞—Ö–Ω–∞—Ö–º–µ \"or None\". –°–µ–≥–∞ –≤—Ä—ä—â–∞ [] –∞–∫–æ –µ –ø—Ä–∞–∑–Ω–æ.\n",
        "                    \"title\": chunk.meta.headings[0] if chunk.meta.headings else \"\", # FIX: \"\" –≤–º–µ—Å—Ç–æ None –∑–∞ –ø–æ-—Å–∏–≥—É—Ä–Ω–æ\n",
        "                },\n",
        "            }\n",
        "            for chunk in chunks\n",
        "        ]\n",
        "\n",
        "        # 4. Insert into LanceDB\n",
        "        print(\"   üöÄ Indexing to LanceDB...\")\n",
        "        table.add(processed_chunks)\n",
        "        print(\"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–∞–Ω!\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå –ì–†–ï–®–ö–ê –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞—Ç–∞ –Ω–∞ —Ñ–∞–π–ª–∞: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- MAIN LOOP ---\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(INPUT_FOLDER):\n",
        "        os.makedirs(INPUT_FOLDER)\n",
        "        print(f\"üìÅ –°—ä–∑–¥–∞–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ '{INPUT_FOLDER}'. –°–ª–∞–≥–∞–π —Ñ–∞–π–ª–æ–≤–µ –≤—ä—Ç—Ä–µ (PDF, DOCX, PPTX, XLSX)!\")\n",
        "\n",
        "    tokenizer = OpenAITokenizerWrapper(model_name=\"cl100k_base\", max_length=8191)\n",
        "    table = get_or_create_table()\n",
        "\n",
        "    print(f\"\\nüëÄ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞ –ø–∞–ø–∫–∞ '{INPUT_FOLDER}'...\")\n",
        "    print(\"–ù–∞—Ç–∏—Å–Ω–∏ Ctrl+C –∑–∞ —Å–ø–∏—Ä–∞–Ω–µ.\\n\")\n",
        "\n",
        "    # –°–ø–∏—Å—ä–∫ —Å –ø–æ–¥–¥—ä—Ä–∂–∞–Ω–∏ —Ñ–æ—Ä–º–∞—Ç–∏\n",
        "    extensions = [\"*.pdf\", \"*.docx\", \"*.pptx\", \"*.html\", \"*.md\", \"*.xlsx\"]\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            files_to_process = []\n",
        "            for ext in extensions:\n",
        "                files_to_process.extend(glob.glob(os.path.join(INPUT_FOLDER, ext)))\n",
        "\n",
        "            if files_to_process:\n",
        "                print(f\"üîé –ù–∞–º–µ—Ä–µ–Ω–∏ {len(files_to_process)} —Ñ–∞–π–ª–∞ –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞!\")\n",
        "\n",
        "                for file_path in files_to_process:\n",
        "                    success = process_single_file(file_path, table, tokenizer)\n",
        "\n",
        "                    if success:\n",
        "                        try:\n",
        "                            os.remove(file_path)\n",
        "                            print(f\"   üóëÔ∏è –ò–∑—Ç—Ä–∏—Ç –æ—Ç Input: {os.path.basename(file_path)}\")\n",
        "                        except OSError as e:\n",
        "                            print(f\"   ‚ö†Ô∏è –ù–µ –º–æ–≥–∞ –¥–∞ –∏–∑—Ç—Ä–∏—è —Ñ–∞–π–ª–∞: {e}\")\n",
        "                    else:\n",
        "                        new_name = file_path + \".failed\"\n",
        "                        try:\n",
        "                            os.rename(file_path, new_name)\n",
        "                            print(f\"   ‚ö†Ô∏è –ü—Ä–µ–∏–º–µ–Ω—É–≤–∞–Ω –Ω–∞ {os.path.basename(new_name)}\")\n",
        "                        except:\n",
        "                            pass # –ê–∫–æ –∏ –ø—Ä–µ–∏–º–µ–Ω—É–≤–∞–Ω–µ—Ç–æ –Ω–µ —Å—Ç–∞–Ω–µ, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥—ä–ª–∂–∞–≤–∞–º–µ\n",
        "\n",
        "            time.sleep(10)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüõë –°–ø–∏—Ä–∞–Ω–µ –Ω–∞ —Å–∫—Ä–∏–ø—Ç–∞. –î–æ–≤–∏–∂–¥–∞–Ω–µ!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpToUwZTx4Ne",
        "outputId": "bdce472d-190a-4f02-fcf1-fb9e7e785ae8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ –°—ä–∑–¥–∞–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ 'INPUT'. –°–ª–∞–≥–∞–π —Ñ–∞–π–ª–æ–≤–µ –≤—ä—Ç—Ä–µ (PDF, DOCX, PPTX, XLSX)!\n",
            "\n",
            "üëÄ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞ –ø–∞–ø–∫–∞ 'INPUT'...\n",
            "–ù–∞—Ç–∏—Å–Ω–∏ Ctrl+C –∑–∞ —Å–ø–∏—Ä–∞–Ω–µ.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2025-11-22 23:00:27,930 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 23:00:27,975 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 23:00:27,976 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé –ù–∞–º–µ—Ä–µ–Ω–∏ 1 —Ñ–∞–π–ª–∞ –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞!\n",
            "\n",
            "üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞: BOFI 50014.1 –ü—Ä–∏–≥–æ—Ç–≤—è–Ω–µ –∏ –∑–∞–ø—ä–ª–≤–∞–Ω–µ –Ω–∞ –∏–∑–¥–µ–ª–∏—è—Ç–∞ —Å –≥–∞–∑–æ–±–µ—Ç–æ–Ω–Ωa —Å–º–µ—Å(3).pdf\n",
            "   üîÑ –§–∞–π–ª—ä—Ç 'BOFI 50014.1 –ü—Ä–∏–≥–æ—Ç–≤—è–Ω–µ –∏ –∑–∞–ø—ä–ª–≤–∞–Ω–µ –Ω–∞ –∏–∑–¥–µ–ª–∏—è—Ç–∞ —Å –≥–∞–∑–æ–±–µ—Ç–æ–Ω–Ωa —Å–º–µ—Å(3).pdf' –≤–µ—á–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞. –ò–∑—Ç—Ä–∏–≤–∞–Ω–µ –Ω–∞ —Å—Ç–∞—Ä–∏—Ç–µ –∑–∞–ø–∏—Å–∏...\n",
            "   üóëÔ∏è –°—Ç–∞—Ä–∏—Ç–µ –∑–∞–ø–∏—Å–∏ —Å–∞ –∏–∑—Ç—Ä–∏—Ç–∏. –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ –Ω–æ–≤–∞—Ç–∞ –≤–µ—Ä—Å–∏—è...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2025-11-22 23:00:28,219 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 23:00:28,223 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 23:00:28,225 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 23:00:28,336 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 23:00:28,416 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-11-22 23:00:28,417 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[33m[WARNING] 2025-11-22 23:00:33,653 [RapidOCR] main.py:123: The text detection result is empty\u001b[0m\n",
            "\u001b[33m[WARNING] 2025-11-22 23:00:36,392 [RapidOCR] main.py:123: The text detection result is empty\u001b[0m\n",
            "\u001b[33m[WARNING] 2025-11-22 23:00:37,178 [RapidOCR] main.py:123: The text detection result is empty\u001b[0m\n",
            "\u001b[33m[WARNING] 2025-11-22 23:00:41,456 [RapidOCR] main.py:123: The text detection result is empty\u001b[0m\n",
            "\u001b[33m[WARNING] 2025-11-22 23:00:42,151 [RapidOCR] main.py:123: The text detection result is empty\u001b[0m\n",
            "\u001b[33m[WARNING] 2025-11-22 23:00:42,287 [RapidOCR] main.py:123: The text detection result is empty\u001b[0m\n",
            "\u001b[33m[WARNING] 2025-11-22 23:00:43,100 [RapidOCR] main.py:123: The text detection result is empty\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   üß© –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–∏ 32 –ø–∞—Ä—á–µ—Ç–∞.\n",
            "   üöÄ Indexing to LanceDB...\n",
            "   ‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–∞–Ω!\n",
            "   üóëÔ∏è –ò–∑—Ç—Ä–∏—Ç –æ—Ç Input: BOFI 50014.1 –ü—Ä–∏–≥–æ—Ç–≤—è–Ω–µ –∏ –∑–∞–ø—ä–ª–≤–∞–Ω–µ –Ω–∞ –∏–∑–¥–µ–ª–∏—è—Ç–∞ —Å –≥–∞–∑–æ–±–µ—Ç–æ–Ω–Ωa —Å–º–µ—Å(3).pdf\n",
            "üîé –ù–∞–º–µ—Ä–µ–Ω–∏ 1 —Ñ–∞–π–ª–∞ –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞!\n",
            "\n",
            "üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞: MOST  –°—Ç–∞–Ω–¥–∞—Ä—Ç —Ä–∞—Å—á–µ—Ç–∞ –Ω–æ—Ä–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ (–Ω–µ—É—Ç–≤).docx\n",
            "   üß© –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–∏ 11 –ø–∞—Ä—á–µ—Ç–∞.\n",
            "   üöÄ Indexing to LanceDB...\n",
            "   ‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–∞–Ω!\n",
            "   üóëÔ∏è –ò–∑—Ç—Ä–∏—Ç –æ—Ç Input: MOST  –°—Ç–∞–Ω–¥–∞—Ä—Ç —Ä–∞—Å—á–µ—Ç–∞ –Ω–æ—Ä–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ (–Ω–µ—É—Ç–≤).docx\n",
            "\n",
            "üõë –°–ø–∏—Ä–∞–Ω–µ –Ω–∞ —Å–∫—Ä–∏–ø—Ç–∞. –î–æ–≤–∏–∂–¥–∞–Ω–µ!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}